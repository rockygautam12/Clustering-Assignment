{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84345f8-8c22-4e7f-bee8-4edef149b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: What is the difference between K-Means and Hierarchical Clustering?\n",
    "Provide a use case for each.\n",
    "\n",
    "                  K-Means Clustering\n",
    "\n",
    "Definition: A partition-based clustering algorithm that divides data into k clusters by minimizing the variance within clusters.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Choose k (number of clusters).\n",
    "\n",
    "Randomly initialize cluster centroids.\n",
    "\n",
    "Assign each point to the nearest centroid.\n",
    "\n",
    "Recompute centroids until convergence.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Fast and scalable for large datasets.\n",
    "\n",
    "Works well with spherical and well-separated clusters.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Must predefine k.\n",
    "\n",
    "Sensitive to outliers and initialization.\n",
    "\n",
    "Struggles with non-spherical clusters.\n",
    "\n",
    "✅ Use Case:\n",
    "Customer segmentation in marketing (e.g., grouping shoppers into 5 clusters based on purchase behavior).\n",
    "\n",
    "Hierarchical Clustering\n",
    "\n",
    "Definition: A tree-based clustering algorithm that builds a hierarchy (dendrogram) of clusters.\n",
    "\n",
    "Types:\n",
    "\n",
    "Agglomerative (bottom-up): Start with each point as a cluster, merge iteratively.\n",
    "\n",
    "Divisive (top-down): Start with all points in one cluster, split iteratively.\n",
    "\n",
    "Pros:\n",
    "\n",
    "No need to predefine k.\n",
    "\n",
    "Produces a dendrogram for flexible cluster selection.\n",
    "\n",
    "Good for smaller datasets and discovering nested structures.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Computationally expensive (O(n²) or worse).\n",
    "\n",
    "Not suitable for very large datasets.\n",
    "\n",
    "✅ Use Case:\n",
    "Document clustering in text mining (e.g., building a hierarchy of news articles by topic/subtopic).\n",
    "\n",
    "Key Difference in Simple Words\n",
    "\n",
    "K-Means: \"Flat\" clustering → you tell it how many groups you want.\n",
    "\n",
    "Hierarchical: \"Tree\" clustering → it shows relationships among clusters and lets you decide later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9c8d1-c332-4ea9-ae39-84213510283a",
   "metadata": {},
   "source": [
    "Question 2: Explain the purpose of the Silhouette Score in evaluating clustering\n",
    "algorithms\n",
    "\n",
    "Silhouette Score in Clustering\n",
    "Purpose\n",
    "\n",
    "The Silhouette Score is a metric used to evaluate the quality of clusters formed by a clustering algorithm.\n",
    "It measures how well each data point fits within its assigned cluster compared to other clusters.\n",
    "\n",
    "How It Works\n",
    "\n",
    "For each data point i:\n",
    "\n",
    "a(i): Average distance of point i to all other points in the same cluster (cohesion).\n",
    "\n",
    "b(i): Minimum average distance of point i to all points in the nearest other cluster (separation).\n",
    "\n",
    "The Silhouette Score for a point is:\n",
    "\n",
    "\n",
    "Why It’s Useful\n",
    "\n",
    "Helps choose the optimal number of clusters (k) in algorithms like K-Means.\n",
    "\n",
    "Provides an objective measure of cluster quality without requiring ground truth labels.\n",
    "\n",
    "Balances cohesion (tight clusters) and separation (clear distance between clusters).\n",
    "\n",
    "Example\n",
    "\n",
    "Suppose you run K-Means with k=2, 3, 4 on customer data.\n",
    "\n",
    "You compute Silhouette Scores:\n",
    "\n",
    "k=2 → 0.45\n",
    "\n",
    "k=3 → 0.62 ✅ (best separation + cohesion)\n",
    "\n",
    "k=4 → 0.39\n",
    "\n",
    "→ You’d choose k=3 as the optimal number of clusters.\n",
    "\n",
    "✨ In short: The Silhouette Score tells you how natural and well-separated your clusters are, and helps in validating and selecting the best clustering solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dbd05b-4a3a-4fd5-8c2e-5142bd98bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What are the core parameters of DBSCAN, and how do they influence the\n",
    "clustering process?\n",
    "\n",
    "Core Parameters of DBSCAN\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) relies mainly on two parameters:\n",
    "\n",
    "1. ε (epsilon / eps)\n",
    "\n",
    "Definition: The maximum distance between two points for them to be considered neighbors.\n",
    "\n",
    "Influence:\n",
    "\n",
    "Small ε → Many small, fragmented clusters + more noise points.\n",
    "\n",
    "Large ε → Fewer, larger clusters (risk of merging distinct groups).\n",
    "\n",
    "Think of ε as the \"radius of a neighborhood.\"\n",
    "\n",
    "2. MinPts (minimum points)\n",
    "\n",
    "Definition: The minimum number of points required to form a dense region.\n",
    "\n",
    "Influence:\n",
    "\n",
    "Small MinPts → Even small dense regions become clusters → risk of false clusters.\n",
    "\n",
    "Large MinPts → Only very dense regions form clusters → more noise, fewer clusters.\n",
    "\n",
    "Rule of thumb: MinPts ≥ dimension of data + 1 (e.g., for 2D, MinPts ≥ 3).\n",
    "\n",
    "How They Work Together\n",
    "\n",
    "Core Point: A point with at least MinPts neighbors within radius ε.\n",
    "\n",
    "Border Point: A point within ε of a core point but with fewer than MinPts itself.\n",
    "\n",
    "Noise Point (Outlier): Not a core point and not within ε of any core point.\n",
    "\n",
    "Clusters grow from core points, connecting neighboring points that satisfy ε and MinPts.\n",
    "\n",
    "Example\n",
    "\n",
    "Imagine clustering GPS coordinates of taxis in a city:\n",
    "\n",
    "If ε = 200 meters, MinPts = 10 → You detect only dense areas like taxi stands.\n",
    "\n",
    "If ε = 1 km, MinPts = 3 → You might merge different stands into one large cluster.\n",
    "\n",
    "✅ In short:\n",
    "\n",
    "ε (eps) controls the radius of neighborhoods.\n",
    "\n",
    "MinPts controls the minimum density threshold.\n",
    "\n",
    "Together, they decide what counts as a cluster vs. noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963f3ba-b32f-472d-9014-1eebf5fcdefa",
   "metadata": {},
   "source": [
    "Question 4: Why is feature scaling important when applying clustering algorithms like\n",
    "K-Means and DBSCAN?\n",
    "\n",
    "Why Feature Scaling Matters in Clustering\n",
    "1. Distance-Based Nature of Clustering\n",
    "\n",
    "Both K-Means and DBSCAN rely on distance calculations (usually Euclidean).\n",
    "\n",
    "If features are on very different scales, the larger-scale feature dominates the distance.\n",
    "\n",
    "👉 Example:\n",
    "\n",
    "Suppose you cluster customers using:\n",
    "\n",
    "Income (20,000 – 200,000)\n",
    "\n",
    "Age (18 – 70)\n",
    "\n",
    "Without scaling, Income (range in lakhs) outweighs Age, so clusters form mostly by income, ignoring age.\n",
    "\n",
    "2. Impact on K-Means\n",
    "\n",
    "Centroids are computed using mean values.\n",
    "\n",
    "If one feature has a larger scale, centroids shift more in that dimension.\n",
    "\n",
    "Result → biased clusters that don’t represent true similarity.\n",
    "\n",
    "3. Impact on DBSCAN\n",
    "\n",
    "DBSCAN uses ε (radius) in distance space.\n",
    "\n",
    "If features are unscaled, ε will be too small for one dimension and too large for another.\n",
    "\n",
    "Result → Wrong neighborhood density → clusters break or merge incorrectly.\n",
    "\n",
    "4. How to Scale\n",
    "\n",
    "Standardization (Z-score scaling):\n",
    "\n",
    "𝑧\n",
    "=\n",
    "𝑥\n",
    "−\n",
    "𝜇\n",
    "𝜎\n",
    "z=\n",
    "σ\n",
    "x−μ\n",
    "\t​\n",
    "\n",
    "\n",
    "→ Mean = 0, Std = 1.\n",
    "Useful if data is normally distributed.\n",
    "\n",
    "Min-Max Scaling (Normalization):\n",
    "\n",
    "𝑥\n",
    "′\n",
    "=\n",
    "𝑥\n",
    "−\n",
    "𝑥\n",
    "𝑚\n",
    "𝑖\n",
    "𝑛\n",
    "𝑥\n",
    "𝑚\n",
    "𝑎\n",
    "𝑥\n",
    "−\n",
    "𝑥\n",
    "𝑚\n",
    "𝑖\n",
    "𝑛\n",
    "x\n",
    "′\n",
    "=\n",
    "x\n",
    "max\n",
    "\t​\n",
    "\n",
    "−x\n",
    "min\n",
    "\t​\n",
    "\n",
    "x−x\n",
    "min\n",
    "\t​\n",
    "\n",
    "\t​\n",
    "\n",
    "\n",
    "→ Scales features into [0,1].\n",
    "Useful when distribution is unknown or for bounded features.\n",
    "\n",
    "✅ In short:\n",
    "Feature scaling ensures that all features contribute equally to distance calculations, preventing bias from scale differences, and leading to more meaningful clusters in both K-Means and DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f53ffb-9295-45fd-910a-b7e1b46dc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: What is the Elbow Method in K-Means clustering and how does it help\n",
    "determine the optimal number of clusters?\n",
    "\n",
    "Elbow Method in K-Means\n",
    "Problem\n",
    "\n",
    "In K-Means, you must predefine the number of clusters k. But how do you know the “best” k?\n",
    "\n",
    "Idea of the Elbow Method\n",
    "\n",
    "Run K-Means with different values of k (say 1 → 10).\n",
    "\n",
    "For each k, compute the Within-Cluster Sum of Squares (WCSS):\n",
    "\n",
    "WCSS = total squared distance between each point and its cluster centroid.\n",
    "\n",
    "Lower WCSS = tighter, more compact clusters.\n",
    "\n",
    "As k increases:\n",
    "\n",
    "WCSS always decreases (more clusters = better fit).\n",
    "\n",
    "But the improvement diminishes after a certain point.\n",
    "\n",
    "The \"best\" k is at the elbow point of the curve → the point where adding more clusters does not significantly reduce WCSS.\n",
    "\n",
    "Steps\n",
    "\n",
    "Run K-Means with k = 1, 2, 3 … n.\n",
    "\n",
    "Record WCSS for each k.\n",
    "\n",
    "Plot k (x-axis) vs. WCSS (y-axis).\n",
    "\n",
    "Look for the elbow (sharp bend in curve).\n",
    "\n",
    "Example\n",
    "\n",
    "k=1 → WCSS = 1000\n",
    "\n",
    "k=2 → WCSS = 500\n",
    "\n",
    "k=3 → WCSS = 250\n",
    "\n",
    "k=4 → WCSS = 210\n",
    "\n",
    "k=5 → WCSS = 200\n",
    "\n",
    "👉 From k=3 to k=4, improvement is small → optimal k ≈ 3.\n",
    "\n",
    "Why It’s Useful\n",
    "\n",
    "Provides a visual, intuitive way to pick k.\n",
    "\n",
    "Prevents under-clustering (too few clusters) or over-clustering (too many meaningless clusters).\n",
    "\n",
    "✅ In short:\n",
    "The Elbow Method helps choose the optimal number of clusters in K-Means by finding the point where adding more clusters no longer provides significant benefit in reducing WCSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86271605-bc97-466e-b590-d358dbdecaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset:\n",
    "Use make_blobs, make_moons, and sklearn.datasets.load_wine() as\n",
    "specified.\n",
    "Question 6: Generate synthetic data using make_blobs(n_samples=300, centers=4),\n",
    "apply KMeans clustering, and visualize the results with cluster centers.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2bcda6-cb41-48fb-a39d-755e3c359351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6: KMeans on make_blobs dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# 2. Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# 3. Get cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# 4. Visualization\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.7, marker='X', label='Centers')\n",
    "plt.title(\"KMeans Clustering on make_blobs Data (4 clusters)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ab873-5e91-46a1-ae82-2128bfa2d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Load the Wine dataset, apply StandardScaler , and then train a DBSCAN\n",
    "model. Print the number of clusters found (excluding noise).\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1fd36-5cb0-49ae-8c9f-33cee74e8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7: DBSCAN on Wine dataset\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "\n",
    "# 2. Feature scaling (important for DBSCAN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=5)  # eps chosen empirically\n",
    "labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# 4. Count clusters (exclude noise: label = -1)\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(\"Cluster labels:\", np.unique(labels))\n",
    "print(\"Number of clusters found (excluding noise):\", n_clusters)\n",
    "print(\"Number of noise points:\", list(labels).count(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550ddd5-8cef-43b5-85a6-e275b7451f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster labels: [-1  0  1]\n",
    "Number of clusters found (excluding noise): 2\n",
    "Number of noise points: 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aac255-49b3-418a-8577-fe67e9b00c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Generate moon-shaped synthetic data using\n",
    "make_moons(n_samples=200, noise=0.1), apply DBSCAN, and highlight the outliers in\n",
    "the plot.\n",
    "(Include your Python code and output in the code box below.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58ff50-84b8-464d-99d4-febe4d69696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8: DBSCAN on moon-shaped data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# 1. Generate moon-shaped synthetic data\n",
    "X, _ = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "\n",
    "# 2. Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)  # tuned for moons\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# 3. Plot clusters\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Core clusters\n",
    "plt.scatter(X[labels >= 0, 0], X[labels >= 0, 1], c=labels[labels >= 0],\n",
    "            cmap='viridis', s=50, label=\"Clusters\")\n",
    "\n",
    "# Outliers (label = -1)\n",
    "plt.scatter(X[labels == -1, 0], X[labels == -1, 1],\n",
    "            c='red', s=80, marker='x', label=\"Outliers\")\n",
    "\n",
    "plt.title(\"DBSCAN on Moon-Shaped Data\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9967c6d-9b2f-498b-8be7-12b052a87617",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Load the Wine dataset, reduce it to 2D using PCA, then apply\n",
    "Agglomerative Clustering and visualize the result in 2D with a scatter plot.\n",
    "(Include your Python code and output in the code box below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f87c5-5390-4991-9196-0c54406827eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9: Agglomerative Clustering on Wine dataset (2D PCA visualization)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# 1. Load dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "\n",
    "# 2. Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Dimensionality reduction to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 4. Apply Agglomerative Clustering (let’s assume 3 clusters like Wine dataset)\n",
    "agg_clust = AgglomerativeClustering(n_clusters=3)\n",
    "labels = agg_clust.fit_predict(X_pca)\n",
    "\n",
    "# 5. Visualization\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=labels, cmap='viridis', s=50)\n",
    "plt.title(\"Agglomerative Clustering on Wine Dataset (2D PCA projection)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e3c2f-4475-4993-9bf7-e011876e32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10: You are working as a data analyst at an e-commerce company. The\n",
    "marketing team wants to segment customers based on their purchasing behavior to run\n",
    "targeted promotions. The dataset contains customer demographics and their product\n",
    "purchase history across categories.\n",
    "Describe your real-world data science workflow using clustering:\n",
    "● Which clustering algorithm(s) would you use and why?\n",
    "● How would you preprocess the data (missing values, scaling)?\n",
    "● How would you determine the number of clusters?\n",
    "● How would the marketing team benefit from your clustering analysis?\n",
    "(Include your Python code and output in the code box below.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69b713-f00b-49ba-8aae-9596fd5f0970",
   "metadata": {},
   "source": [
    "1. Choice of Clustering Algorithm\n",
    "\n",
    "K-Means → good for large datasets, interpretable clusters.\n",
    "\n",
    "DBSCAN → useful for detecting outlier customers (rare behaviors).\n",
    "\n",
    "Agglomerative Clustering → for exploratory analysis (dendrograms).\n",
    "\n",
    "✅ I’d start with K-Means (scalable, marketing-friendly clusters), and compare with others.\n",
    "\n",
    "2. Data Preprocessing\n",
    "\n",
    "Missing values → impute (mean for numeric, mode for categorical, or \"Unknown\" category).\n",
    "\n",
    "Categorical features → one-hot encoding (e.g., gender, region).\n",
    "\n",
    "Numerical features → standardize using StandardScaler (since K-Means/DBSCAN rely on distances).\n",
    "\n",
    "Dimensionality reduction (PCA) → for visualization and noise removal.\n",
    "\n",
    "3. Choosing the Number of Clusters\n",
    "\n",
    "Elbow Method (plot WCSS vs. k).\n",
    "\n",
    "Silhouette Score (cluster quality).\n",
    "\n",
    "Business context → e.g., 3–6 segments is usually manageable for marketing.\n",
    "\n",
    "4. Business Value\n",
    "\n",
    "The marketing team can use clusters to:\n",
    "\n",
    "Identify high-value customers (loyalty programs, premium offers).\n",
    "\n",
    "Find price-sensitive customers (discount campaigns).\n",
    "\n",
    "Spot cross-sell opportunities (people buying electronics may buy accessories).\n",
    "\n",
    "Detect dormant customers (send re-engagement promotions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b6bd7-4952-410d-a254-2c955c518fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10: Customer Segmentation Workflow with Clustering\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Simulate customer dataset (demographics + purchases)\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"Age\": np.random.randint(18, 70, 200),\n",
    "    \"Annual_Income\": np.random.randint(20000, 150000, 200),\n",
    "    \"Electronics_Spend\": np.random.randint(0, 5000, 200),\n",
    "    \"Clothing_Spend\": np.random.randint(0, 3000, 200),\n",
    "    \"Grocery_Spend\": np.random.randint(500, 5000, 200)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Preprocess (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# 3. Determine optimal number of clusters using Elbow + Silhouette\n",
    "wcss = []\n",
    "sil_scores = []\n",
    "K = range(2, 10)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "# Plot Elbow\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(K, wcss, marker='o')\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "\n",
    "# Plot Silhouette\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(K, sil_scores, marker='o', color='green')\n",
    "plt.title(\"Silhouette Scores\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Train final KMeans model (say k=4 chosen)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df[\"Cluster\"] = labels\n",
    "\n",
    "# 5. PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=labels, cmap=\"viridis\", s=50)\n",
    "plt.title(\"Customer Segmentation (KMeans Clustering, 2D PCA projection)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Inspect clusters\n",
    "print(df.groupby(\"Cluster\").mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e410ab-51b7-4d61-90ad-878cf043588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Expected Output\n",
    "\n",
    "Elbow + Silhouette plots → helps decide best k (say k=4).\n",
    "\n",
    "2D PCA scatter plot → visual clusters of customers.\n",
    "\n",
    "Cluster profiles table → marketing sees:\n",
    "\n",
    "Cluster 0: Young, low income → budget-friendly promotions.\n",
    "\n",
    "Cluster 1: Older, high income → luxury offers.\n",
    "\n",
    "Cluster 2: Middle-aged, family spenders → grocery/household deals.\n",
    "\n",
    "Cluster 3: Tech enthusiasts → electronics campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feccf80-b448-4c87-bb9e-9fff9b8d7862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
